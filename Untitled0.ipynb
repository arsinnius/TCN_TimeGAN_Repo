{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPFFblj5OzIcPhnWWue7pvq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **TCN_TimeGAN_Repo**\n","Adapted from the excellent paper by Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar:  \n","[Time-series Generative Adversarial Networks](https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks),  \n","Neural Information Processing Systems (NeurIPS), 2019.\n","\n","- Last updated Date: April 24th 2020\n","- [Original code](https://bitbucket.org/mvdschaar/mlforhealthlabpub/src/master/alg/timegan/) author: Jinsung Yoon (jsyoon0823@gmail.com)"],"metadata":{"id":"v1jfuQILxi8_"}},{"cell_type":"markdown","source":["# **Description**\n","The model uses Tensorflow 2. In the original timegan paper, the autoencoder was built using an RNN. This project replaces the RNN with the TCN-AE autoencoder described by Thill et al. (2021) in \"Temporal convolutional autoencoder for unsupervised anomaly detection in time series.\"\n","\n","The modified model was tested on two different time series - financial and climate. Using the default settings, TCN-AE outperformed the RNN on the financial time series. However it didn't perform very well on the climate data. The belief is that the performance on both series could be vastly improved if the parameters of the TCN-AE were individually optimized for each time series type."],"metadata":{"id":"DOSLdnWAxozd"}},{"cell_type":"markdown","source":["# **Getting Started**\n","The code is executed in Jupyter notebooks. It was written and tested using Google Colab. At the top of each notebook is four alternating cells – markdown, code, markdown, code. The first markdown cell is “Colab Code” and the following code cell mounts the google colab drive. The second markdown is “Start Folder” and the following code cell changes  the directory to the repo directory. The cells can easily be relabeled and rewritten to accommodate other platforms."],"metadata":{"id":"D2QFDgCmzGmK"}},{"cell_type":"markdown","source":["# **Installing**\n","Under the markdown header cell “Installs” there are installs applicable to the colab environment. These can be modified to accommodate other environments or removed completely if not required."],"metadata":{"id":"fXNqzCRTzUx3"}},{"cell_type":"markdown","source":["# **Repo Structure**"],"metadata":{"id":"DN1QINBYYzRM"}},{"cell_type":"markdown","source":["Three folders are added to the standard .git folder and github files – `experiment_08`, `experiment_09` and `src`.\n","![Repo_dir.png](https://drive.google.com/uc?id=1JYYiXTRsJJxVsP_dJ5Jr9GSZnbxBDDP-)\n","<figcaption></figcaption>\n","\n"],"metadata":{"id":"sJxrq58IYrwz"}},{"cell_type":"markdown","source":["The `src` directory is shown below.\n","\n","![src_dir.png](https://drive.google.com/uc?id=1KjAUkORKK8kpgZpsMYaJHqac-Vjei-xq)\n","<figcaption></figcaption>"],"metadata":{"id":"era3JrtK0AsI"}},{"cell_type":"markdown","source":["The `src` folder contains the files necessary to run the experiments. \n","1. `params_08` – parameter settings for `Financial_TS.ipynb` (experiment 08).\n","2. `params_09` – parameter settings for `Climate_TS.ipynb` (experiment 09).\n","3. `tcn_ae.py` – code for the TCN autoencode.\n","4. `train_fns.py` – training functions for the component models of TCN_TimeGAN."],"metadata":{"id":"jT6f6-9i0Xt3"}},{"cell_type":"markdown","source":["`experiment_08` contains the synthetic financial time series experiment and `experiment_09`, the climate time series experiment. The `experiment_##` folders are organized similarly. For example, the following shows `experiment_08`.\n","![experiment_08_dir.png](https://drive.google.com/uc?id=1s999CvdjmERAsAlDn6AJ3r4KorJ9XpUc)\n","<figcaption></figcaption>"],"metadata":{"id":"iXl6QKUe08UO"}},{"cell_type":"markdown","source":["Notice that `experiment_08` contains three Jupyter notebooks:\n","1. `Financial_TS.ipynb` – Creates the synthesizer that will be used to generate the synthetic time series.\n","2. `Eval_08.ipynb` – runs tests to evaluate the quality of the synthetic time series produced.\n","3. `New_Synthetic_Generation_08.ipynb` – Uses the files created by Financial_TS.ipynb to generate more synthetic time series.\n","\n","\n"],"metadata":{"id":"st0XhGsn1NiC"}},{"cell_type":"markdown","source":["Notice that `experiment_08` contains three Jupyter notebooks:\n","\n","1.   `Financial_TS.ipynb` – Creates the synthesizer that will be used to\n","generate the synthetic time series.\n","2.   `Eval_08.ipynb` – runs tests to evaluate the quality of the synthetic time series produced\n","3. `New_Synthetic_Generation_08.ipynb` – Uses the files created by Financial_TS.ipynb to generate more synthetic time series\n","    "],"metadata":{"id":"Lpihunp-b3Zx"}},{"cell_type":"markdown","source":["In addition, it contains the following folders:\n","1. `autoencoder` – contains the autoencoder model saved in Tensorflow 2 format.\n","2. `data` – contains raw time series data or may be empty if the data is read in directly, for example by `pandas_data_reader`.\n","3. `eval` – contains the evaluation results produced by `eval_08.ipynb`.\n","4. `events` – contains the events recorded by the logger `tf.summary.create_file_writer`.\n","5. `samples` – contains synthetic data samples created by `Financial_TS.ipynb`\n","6. `supervisor` – contains the supervisor model saved in Tensorflow 2 format.    \n","7. `autoencoder` – contains the autoencoder model saved in Tensorflow 2 format."],"metadata":{"id":"8XBJT-IOfX4x"}},{"cell_type":"markdown","source":["The src folder contains the files necessary to run the experiments. The directory is shown below.\n","![src_dir.png](https://drive.google.com/uc?id=1KjAUkORKK8kpgZpsMYaJHqac-Vjei-xq)\n","<figcaption></figcaption>\n"],"metadata":{"id":"AkgQyOn4qejJ"}},{"cell_type":"markdown","source":["1. `params_08` – parameter settings for `Financial_TS.ipynb`.\n","2. `params_09` – parameter settings for `Climate_TS.ipynb`.\n","3. `tcn_ae.py` – code for the TCN autoencode.\n","4. `train_fns.py` – training functions for the component models of TCN_TimeGAN."],"metadata":{"id":"9Nsnea0yqv47"}},{"cell_type":"markdown","source":["# **Executing the code**"],"metadata":{"id":"HE8ZsPrU4z28"}},{"cell_type":"markdown","source":["The code is executed in Jupyter notebooks. The notebooks are structured to be tutorial and are meant to be used in order. After loading and initializing the notebook , the user can either run all cells and go to the end to see the results or step through cell-by-cell to examine model structures, check on tensor shapes, watch values change, etc. experiment_08 is illustrative.\n"," \n","a. `Financial_TS.ipynb` \n","\n","**Outline**\n","1. Title\n","2. Colab Code\n","3. Installs\n","4. Imports & Settings\n","5. Experiment Path\n","6. Prepare Data\n","7. TimeGAN Components\n","8. TimeGAN Training\n","  * Phase 1: Autoencoder Training\n","  * Phase 2: Supervised Training\n","  * Joint Training\n","9. Generate Synthetic Data\n","\n","At the end random samples of real and synthetic time series are printed on the same graph so the user can see if the synthetic data “looks right.” The samples are also saved in the `samples` folder.\n"],"metadata":{"id":"5ZHp5Ht2QYyV"}},{"cell_type":"markdown","source":["\n","b. `eval_08.ipynb`\n","\n","**Outline**\n","1. Title\n","2. Colab Code\n","3. Sample to be Evaluated\n","4. Imports\n","5. Parameters\n","6. Experiment Path\n","7. Load Real and Synthetic Data\n","8. Prepare Sample\n","9. Visualization in 2D: A Qualitative Assessment of Diversity\n","10. Time Series Classification: A Quantitative Assessment of Fidelity\n","11. Train on Synthetic, Test on Real: Assessing Usefulness\n","\n","After generating synthetic samples, the user may evaluate the quality of the synthetic data. There are three kinds of evaluations done:\n","1. **Visualization in 2D** – the notebook plots the data using two different dimensionality reduction algorithms – principal components analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE). PCA is inherently linear. t_SNE is a nonlinear dimensionality reduction technique. That means the algorithm allows the separation of data that cannot be separated by a straight line.\n","2. **Time Series Classification** – the notebook creates a simple classifier, calculates the metrics accuracy and AUC (area under the curve) and compares the results for the real and synthetic time series. The synthetic data should fool the classifier 50% of the time. That is, the classifier should be no better than random guessing.\n","3. **Train on Synthetic, Test on Real (TSTR)** - TSTR vs. TRTR (Train on Real, Test on Real). The notebook creates a regression model that tries to predict the last time step for the six stock prices. It plots the Mean Absolute Error (MAE) using TSTR and TRTR and compares the results. The synthetic data is useful if the performance using TSTR is equal to or better than using TRTR. Results are saved in the `evals` folder.\n","\n","**NOTE!! These calculations are for illustration only!!!** The notebook calculates tests using a single sample. A rigorous evaluation would involve performing the tests on many (thousands of) combined samples. The notebook can be readily modified to accomplish this."],"metadata":{"id":"5Th3ArNBQgYr"}},{"cell_type":"markdown","source":["c. `New_Synthetic_Generation_08.ipynb`\n","\n","**Outline**\n","1. Title\n","2. Colab Code\n","3. Installs\n","4. Samples to be Generated\n","5. Imports\n","6. Parameters\n","7. Get Data\n","8. Get Synthesizer\n","9. Prepare Data\n","10. Plot Sample Series\n","\n","This notebook can be used to generate new synthetic data using the models and files created by  `Financial_TS.ipynb`. Following the markdown cell “Installs”, is the markdown “Samples to be Generated” followed by a code cell containing two variables which the user must assign values – `n_init` (starting sample number) and `n_plots` (number of samples). Suppose the user assigns the values 11 and 5 respectively. Then the notebook will generate 5 samples (`sample_11` to `sample_15`) and place them in the samples folder. To avoid overwriting samples currently in that folder, the use must make sure that the numbers don’t overlap. For example, one could use 1-100 for one set, 101-200 for another and so on. Clearly, the notebook in its present form is for illustrative purposes.\n","\n","In reality, after perhaps many rounds of generating, evaluating  and tweaking the code, the user may wish to create thousands (millions) of synthetic time series to be used for training an AI/ML model. This notebook would have to be modified. Luckily, the Jupyter notebook format is flexible. For example, rather than storing samples in the samples folder, they could be written to an `h5` file and every *n*th sample printed to indicate progress."],"metadata":{"id":"s1FFjKn-RDXJ"}},{"cell_type":"markdown","source":["# **Issues**\n","\n","1. The TCN was tested using default settings. It outperformed on the financial data but didn’t perform well on the climate data. Can the parameters of the TCN be optimized separately for the two types of time series?\n","2. More generally, can a user-guide be written to explain how to optimize TCN parameters based on time series characteristics for any time series?"],"metadata":{"id":"WvpcVvD9UQ2R"}},{"cell_type":"markdown","source":["# **License**\n","This project is licensed under the MIT License - see [MIT License](https://opensource.org/licenses/MIT) for details "],"metadata":{"id":"0rQ5BEcmXKNM"}},{"cell_type":"markdown","source":["# **Acknowledgements**\n","\n","1. [Machine Learning for Algorithmic Trading](https://www.amazon.com/Machine-Learning-Algorithmic-Trading-alternative/dp/1839217715/ref=sr_1_1?crid=2EOY5VIDNELLR&keywords=Jansen%2C+%22Machine+Learning%22&qid=1668352381&s=books&sprefix=jansen%2C+machine+learning+%2Cstripbooks%2C139&sr=1-1), 2nd edition by Stefan Jansen\n","\n","2. Thill, Konen, Wang and Back (2021), [Temporal convolutional autoencoder for unsupervised anomaly detection in time series](https://www.sciencedirect.com/science/article/abs/pii/S1568494621006724)\n","\n","3. Yoon, Jarrett and van der Schaar (2019), [Time Series Generative Adversarial Networks](https://proceedings.neurips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf)\n"],"metadata":{"id":"P3YfoS5gZPaG"}}]}